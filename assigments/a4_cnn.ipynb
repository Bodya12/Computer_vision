{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Colormap of the notebook:*\n",
    "\n",
    "* <span style=\"color:red\">assignment problem</span>. The red color indicates the task that should be done\n",
    "* <span style=\"color:green\">debugging</span>. The green tells you what is expected outcome. Its primarily goal to help you get the correct answer\n",
    "* <span style=\"color:blue\">hints</span>.\n",
    "\n",
    "Assignment 4 (Convolutional Neural Network)\n",
    "======================\n",
    "\n",
    "partially based on 'PyTorch.ipynb' from assignment 2 in cs231 course.  \n",
    "http://cs231n.github.io/assignments2017/assignment2/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch supports many layer types, loss functions, and optimizers\n",
    "\n",
    "* Layers: http://pytorch.org/docs/nn.html\n",
    "* Activations: http://pytorch.org/docs/nn.html#non-linear-activations\n",
    "* Loss functions: http://pytorch.org/docs/nn.html#loss-functions\n",
    "* Optimizers: http://pytorch.org/docs/optim.html#algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for compatability issues \n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to make interactive plotting possible\n",
    "%matplotlib inline\n",
    "# for auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make plots a bit nicer\n",
    "plt.matplotlib.rcParams.update({'font.size': 18, 'font.family': 'serif'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random seed settings\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# data type\n",
    "dtype_np = np.float64\n",
    "dtype_torch = torch.FloatTensor\n",
    "dtype_torch_cuda = torch.cuda.FloatTensor # to run on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.data_set import DataSetCifar10, DataSetDTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mykola/Documents/LvDSS2017/ComputerVision/LDSSS2017/assigments/data\r\n",
      "file exists\r\n"
     ]
    }
   ],
   "source": [
    "# do it for sanity check (that you have a dataset)\n",
    "!./data/get_cifar10_dataset.sh # for cifar10\n",
    "#!./data/get_dtd_dataset.sh # for dtd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_data = 'data' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set = DataSetCifar10(path_data, num_dunkeys=4, batch_size=batch_size)\n",
    "#data_set = DataSetDTD(path_data, num_dunkeys=4, batch_size=100, fin_scale=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_test(data_loader, model_current, train_test, gpu=False):\n",
    "    \"\"\"\n",
    "     Giving dataset and model function calculates and prints out the accuracy\n",
    "    \n",
    "    Args:\n",
    "        data_loader (DataLoader): loaded dataset\n",
    "        model_current (model): the current model\n",
    "        train_test (string): either 'train' or 'test' to define on which of these datasets we calculate accuracy\n",
    "        gpu (bool): flag to indicate if our model runs on gpu\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images_, labels_ in data_loader[train_test]:\n",
    "        \n",
    "        if gpu:\n",
    "            images_ = Variable(images_.type(dtype_torch_cuda), volatile=True)\n",
    "            outputs_ = model_current(images_)\n",
    "            _, predicted = torch.max(outputs_.data.cpu(), 1)\n",
    "        else:\n",
    "            images_ = Variable(images_)\n",
    "            outputs_ = model_current(images_)\n",
    "            _, predicted = torch.max(outputs_.data, 1)\n",
    "            \n",
    "        total += labels_.size(0)\n",
    "        correct += (predicted == labels_).sum()\n",
    "    print('accuracy[' + train_test + '] : %f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that our image data (and intermediate feature maps) are initially N x C x H x W, where:\n",
    "* N is the number of datapoints\n",
    "* C is the number of channels\n",
    "* H is the height of the intermediate feature map in pixels\n",
    "* W is the height of the intermediate feature map in pixels\n",
    "\n",
    "we use a \"Flatten\" operation to collapse the C x H x W values per representation into a single long vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size() # read in N, C, H, W\n",
    "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential (\n",
       "  (0): Conv2d(3, 32, kernel_size=(7, 7), stride=(2, 2))\n",
       "  (1): ReLU (inplace)\n",
       "  (2): Flatten (\n",
       "  )\n",
       "  (3): Linear (5408 -> 10)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's where we define the architecture of the model... \n",
    "first_model = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, kernel_size=7, stride=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                Flatten(), # see above for explanation\n",
    "                nn.Linear(5408, 10), # affine layer\n",
    "              )\n",
    "\n",
    "# Set the type of all data in this model to be FloatTensor \n",
    "first_model.type(dtype_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = copy.deepcopy(first_model).type(dtype_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().type(dtype_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [1/1], step: [100/500], loss: 1.8933\n",
      "epoch: [1/1], step: [200/500], loss: 1.5111\n",
      "epoch: [1/1], step: [300/500], loss: 1.4503\n",
      "epoch: [1/1], step: [400/500], loss: 1.5622\n",
      "epoch: [1/1], step: [500/500], loss: 1.5233\n",
      "--- epoch: [1, 1]\n",
      "accuracy[test] : 47.270000 %\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "t = 0\n",
    "logger = {}\n",
    "logger['iteration'] = []\n",
    "logger['loss_iteration'] = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(data_set.loader['train']):\n",
    "        # get data to train\n",
    "        images = Variable(images.type(dtype_torch))\n",
    "        labels = Variable(labels.type(dtype_torch).long())\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model.forward(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # reporting & logging\n",
    "        logger['iteration'] += [t]\n",
    "        logger['loss_iteration'] += [loss.data[0]]\n",
    "        t += 1\n",
    "        if t % 100 == 0:\n",
    "            print('epoch: [%d/%d], step: [%d/%d], loss: %.4f' %\n",
    "                   (epoch + 1, num_epochs, i+1, len(data_set.dataset['train'])//batch_size, loss.data[0]))\n",
    "        \n",
    "    print('--- epoch: [%d, %d]' % (epoch + 1, num_epochs))\n",
    "    #make_test(data_set.loader, model, 'train')\n",
    "    make_test(data_set.loader, model, 'test')\n",
    "\n",
    "    # switch back to the training  mode\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoUAAAFQCAYAAADJMgTVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1FCy2wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb5da0b3fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(logger['iteration'], logger['loss_iteration'],'ob', label=\"loss\")\n",
    "\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that CUDA is properly configured and you have a GPU available\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_gpu = copy.deepcopy(model).type(dtype_torch_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variable for CPU ( as usual )\n",
    "x = torch.randn(128, 3, 32, 32).type(dtype_torch)\n",
    "x_var = Variable(x.type(dtype_torch)) # Construct a PyTorch Variable out of your input data\n",
    "\n",
    "# and now for GPU\n",
    "x_gpu = torch.randn(128, 3, 32, 32).type(dtype_torch_cuda)\n",
    "x_var_gpu = Variable(x.type(dtype_torch_cuda)) # Construct a PyTorch Variable out of your input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 6.49 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "yyy = model(x_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 416.86 times longer than the fastest. This could mean that an intermediate result is being cached \n",
      "1 loops, best of 3: 1.46 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "torch.cuda.synchronize() # make sure there are no pending GPU computations\n",
    "yyy = model_gpu(x_var_gpu) \n",
    "torch.cuda.synchronize() # make sure there are no pending GPU computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion_gpu = nn.CrossEntropyLoss().type(dtype_torch_cuda)\n",
    "optimizer_gpu = torch.optim.Adam(model_gpu.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [1/10], step: [100/500], loss: 10661.1104\n",
      "epoch: [1/10], step: [200/500], loss: 37117.7773\n",
      "epoch: [1/10], step: [300/500], loss: 64515.1016\n",
      "epoch: [1/10], step: [400/500], loss: 131198.7656\n",
      "epoch: [1/10], step: [500/500], loss: 185517.4531\n",
      "--- epoch: [1, 10]\n",
      "accuracy[test] : 14.540000 %\n",
      "epoch: [2/10], step: [100/500], loss: 327276.4688\n",
      "epoch: [2/10], step: [200/500], loss: 253440.9531\n",
      "epoch: [2/10], step: [300/500], loss: 314021.1875\n",
      "epoch: [2/10], step: [400/500], loss: 275305.3750\n",
      "epoch: [2/10], step: [500/500], loss: 441624.2500\n",
      "--- epoch: [2, 10]\n",
      "accuracy[test] : 17.030000 %\n",
      "epoch: [3/10], step: [100/500], loss: 377274.1875\n",
      "epoch: [3/10], step: [200/500], loss: 244484.3594\n",
      "epoch: [3/10], step: [300/500], loss: 382759.9062\n",
      "epoch: [3/10], step: [400/500], loss: 491383.5938\n",
      "epoch: [3/10], step: [500/500], loss: 411129.1562\n",
      "--- epoch: [3, 10]\n",
      "accuracy[test] : 22.190000 %\n",
      "epoch: [4/10], step: [100/500], loss: 707571.3750\n",
      "epoch: [4/10], step: [200/500], loss: 511280.5312\n",
      "epoch: [4/10], step: [300/500], loss: 841206.1875\n",
      "epoch: [4/10], step: [400/500], loss: 650398.8125\n",
      "epoch: [4/10], step: [500/500], loss: 738193.1875\n",
      "--- epoch: [4, 10]\n",
      "accuracy[test] : 19.770000 %\n",
      "epoch: [5/10], step: [100/500], loss: 618647.5625\n",
      "epoch: [5/10], step: [200/500], loss: 596815.5000\n",
      "epoch: [5/10], step: [300/500], loss: 941804.1875\n",
      "epoch: [5/10], step: [400/500], loss: 732717.9375\n",
      "epoch: [5/10], step: [500/500], loss: 382590.2812\n",
      "--- epoch: [5, 10]\n",
      "accuracy[test] : 23.880000 %\n",
      "epoch: [6/10], step: [100/500], loss: 558393.2500\n",
      "epoch: [6/10], step: [200/500], loss: 675954.3750\n",
      "epoch: [6/10], step: [300/500], loss: 737935.2500\n",
      "epoch: [6/10], step: [400/500], loss: 648920.0625\n",
      "epoch: [6/10], step: [500/500], loss: 811817.4375\n",
      "--- epoch: [6, 10]\n",
      "accuracy[test] : 24.750000 %\n",
      "epoch: [7/10], step: [100/500], loss: 1059777.2500\n",
      "epoch: [7/10], step: [200/500], loss: 809079.1250\n",
      "epoch: [7/10], step: [300/500], loss: 658368.1875\n",
      "epoch: [7/10], step: [400/500], loss: 763580.3750\n",
      "epoch: [7/10], step: [500/500], loss: 1005194.5000\n",
      "--- epoch: [7, 10]\n",
      "accuracy[test] : 24.290000 %\n",
      "epoch: [8/10], step: [100/500], loss: 878607.0625\n",
      "epoch: [8/10], step: [200/500], loss: 769682.3750\n",
      "epoch: [8/10], step: [300/500], loss: 941954.8125\n",
      "epoch: [8/10], step: [400/500], loss: 933387.1250\n",
      "epoch: [8/10], step: [500/500], loss: 876438.7500\n",
      "--- epoch: [8, 10]\n",
      "accuracy[test] : 24.400000 %\n",
      "epoch: [9/10], step: [100/500], loss: 1208298.8750\n",
      "epoch: [9/10], step: [200/500], loss: 1148321.5000\n",
      "epoch: [9/10], step: [300/500], loss: 700376.8750\n",
      "epoch: [9/10], step: [400/500], loss: 1229588.7500\n",
      "epoch: [9/10], step: [500/500], loss: 1204279.8750\n",
      "--- epoch: [9, 10]\n",
      "accuracy[test] : 25.450000 %\n",
      "epoch: [10/10], step: [100/500], loss: 892657.6250\n",
      "epoch: [10/10], step: [200/500], loss: 907784.5625\n",
      "epoch: [10/10], step: [300/500], loss: 1192228.7500\n",
      "epoch: [10/10], step: [400/500], loss: 956768.8125\n",
      "epoch: [10/10], step: [500/500], loss: 1056925.0000\n",
      "--- epoch: [10, 10]\n",
      "accuracy[test] : 26.680000 %\n"
     ]
    }
   ],
   "source": [
    "# train gpu model\n",
    "t = 0\n",
    "logger = {}\n",
    "logger['iteration'] = []\n",
    "logger['loss_iteration'] = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(data_set.loader['train']):\n",
    "        # get data to train\n",
    "        images = Variable(images.type(dtype_torch_cuda))\n",
    "        labels = Variable(labels.type(dtype_torch_cuda).long())\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        optimizer_gpu.zero_grad()\n",
    "        outputs = model_gpu.forward(images)\n",
    "        loss = criterion_gpu(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_gpu.step()\n",
    "\n",
    "        # reporting & logging\n",
    "        logger['iteration'] += [t]\n",
    "        logger['loss_iteration'] += [loss.data[0]]\n",
    "        t += 1\n",
    "        if t % 100 == 0:\n",
    "            print('epoch: [%d/%d], step: [%d/%d], loss: %.4f' %\n",
    "                   (epoch + 1, num_epochs, i+1, len(data_set.dataset['train'])//batch_size, loss.data[0]))\n",
    "        \n",
    "    print('--- epoch: [%d, %d]' % (epoch + 1, num_epochs))\n",
    "    #make_test(data_set.loader, model_gpu, 'train')\n",
    "    make_test(data_set.loader, model_gpu, 'test', gpu=True)\n",
    "\n",
    "    # switch back to the training  mode\n",
    "    model_gpu.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **[PROBLEM]**: </span>  \n",
    "<span style=\"color:red\"> Using the code provided above as guidance, </span>  \n",
    "<span style=\"color:red\"> and using the following PyTorch documentation, specify a model with the following architecture: </span>  \n",
    "\n",
    "<span style=\"color:red\"> - 7x7 Convolutional Layer with 32 filters and stride of 1 </span>  \n",
    "<span style=\"color:red\"> - ReLU Activation Layer </span>  \n",
    "<span style=\"color:red\"> - BatchNorm2D (spatial batch normalization) </span>  \n",
    "<span style=\"color:red\"> - 2x2 Max Pooling layer with a stride of 2 </span>  \n",
    "<span style=\"color:red\"> - Affine layer with 1024 output units </span>  \n",
    "<span style=\"color:red\"> - ReLU Activation Layer </span>  \n",
    "<span style=\"color:red\"> - Affine layer from 1024 input units to 10 outputs </span>  \n",
    "<span style=\"color:red\"> - And finally, set up a **cross-entropy** loss function and the **RMSprop** learning rule. </span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> To make sure you're doing the right thing, use the following tool to check the dimensionality of your output (it should be 64 x 10, since our batches have size 64 and the output of the final affine layer should be 10, corresponding to our 10 classes): </span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_model = nn.Sequential( # You fill this in!\n",
    "                         )\n",
    "\n",
    "model_gpu = copy.deepcopy(new_model).type(dtype_torch_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = torch.randn(64, 3, 32, 32).type(dtype_torch_cuda)\n",
    "x_var = Variable(x.type(dtype_torch_cuda)) \n",
    "ans = model_gpu(x_var)        \n",
    "\n",
    "# Check to make sure what comes out of your model\n",
    "# is the right dimensionality... this should be True\n",
    "# if you've done everything correctly\n",
    "np.array_equal(np.array(ans.size()), np.array([64, 10])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tips from *cs231 course*\n",
    "\n",
    "### Things you should try:\n",
    "- **Filter size**: Above we used 7x7; this makes pretty pictures but smaller filters may be more efficient\n",
    "- **Number of filters**: Above we used 32 filters. Do more or fewer do better?\n",
    "- **Pooling vs Strided Convolution**: Do you use max pooling or just stride convolutions?\n",
    "- **Batch normalization**: Try adding spatial batch normalization after convolution layers and vanilla batch normalization after affine layers. Do your networks train faster?\n",
    "- **Network architecture**: The network above has two layers of trainable parameters. Can you do better with a deep network? Good architectures to try include:\n",
    "    - [conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [batchnorm-relu-conv]xN -> [affine]xM -> [softmax or SVM]\n",
    "- **Global Average Pooling**: Instead of flattening and then having multiple affine layers, perform convolutions until your image gets small (7x7 or so) and then perform an average pooling operation to get to a 1x1 image picture (1, 1 , Filter#), which is then reshaped into a (Filter#) vector. This is used in [Google's Inception Network](https://arxiv.org/abs/1512.00567) (See Table 1 for their architecture).\n",
    "- **Regularization**: Add l2 weight regularization, or perhaps use Dropout.\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and regularization strength. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "\n",
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. \n",
    "\n",
    "- Alternative update steps: For the assignment we implemented SGD+momentum, RMSprop, and Adam; you could try alternatives like AdaGrad or AdaDelta.\n",
    "- Alternative activation functions such as leaky ReLU, parametric ReLU, ELU, or MaxOut.\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "- New Architectures\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385) where the input from the previous layer is added to the output.\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993) where inputs into previous layers are concatenated together.\n",
    "  - [This blog has an in-depth overview](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
